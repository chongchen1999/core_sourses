# Cache的层次结构

Cache（高速缓存）是计算机系统中为了提高数据访问速度而引入的一种硬件机制。它位于CPU和主存储器之间，存储了CPU近期最常访问的数据副本。由于Cache的速度远快于主存，因此可以显著减少CPU等待数据的时间，从而提升系统性能。

Cache通常不是单一的一层，而是多级的层次结构，常见的是L1、L2和L3 Cache。

* **L1 Cache**：位于CPU芯片内部，速度最快，容量最小，通常分为数据缓存（L1d）和指令缓存（L1i）。
* **L2 Cache**：通常也位于CPU芯片内部，比L1 Cache慢一些，容量更大。
* **L3 Cache**：通常位于CPU和主存之间，或作为CPU芯片的一部分，比L2 Cache慢，容量最大。

每一级Cache都比其下一级Cache（主存）更快、更小、更昂贵，但比其上一级Cache更慢、更大、更便宜。

## 1. 什么是Cache Set，Cache Line

理解Cache的运作，需要了解两个基本概念：Cache Line 和 Cache Set。

* **Cache Line（缓存行）**：
    * Cache和主存之间数据传输的最小单位。
    * 当CPU需要一个数据时，即使只需要一个字节，也会将包含该字节的整个Cache Line从主存加载到Cache中。
    * 通常大小为32字节、64字节或128字节，现代处理器多为64字节。
    * 一个Cache Line中包含：
        * **数据块（Data Block）**：实际存储的数据。
        * **标记位（Tag）**：用于标识该Cache Line存储的是主存中的哪个地址块。
        * **有效位（Valid Bit）**：表示该Cache Line中的数据是否有效。
        * **脏位（Dirty Bit）** (仅用于写回策略)：表示该Cache Line中的数据是否被修改过，且尚未写回主存。

* **Cache Set（缓存组）**：
    * Cache被组织成多个组（Set）。
    * 每个组包含多条Cache Line。
    * 当主存中的一个地址块被映射到Cache时，它会被映射到特定的一个Cache Set中。
    * 这种分组的设计是为了在全相联映射和直接映射之间取得平衡，优化查找效率和硬件复杂性。

## 2. Cache的映射策略和替换策略

Cache的映射策略和替换策略是决定数据如何被放置在Cache中以及当Cache满时哪些数据被淘汰的关键。

### Cache的映射策略

映射策略决定了主存中的一个地址块可以被放置在Cache的哪个位置。

* **直接映射（Direct Mapped）**：
    * 最简单的一种映射方式。
    * 主存中的每个块只能映射到Cache中的唯一一个位置。
    * 映射关系通常通过公式 `(主存块地址) mod (Cache中的块数)` 来确定。
    * **优点**：查找速度快，硬件实现简单。
    * **缺点**：容易发生冲突，即使Cache中有空余空间，也可能因为冲突而导致频繁的替换（颠簸现象），降低命中率。

* **全相联映射（Fully Associative）**：
    * 主存中的任何块可以被映射到Cache中的任何一个位置。
    * 查找时需要并行比较所有Cache Line的Tag，找出匹配项。
    * **优点**：灵活性最高，Cache空间利用率最高，冲突概率最低。
    * **缺点**：硬件实现复杂，成本高，查找速度慢（需要并行比较），通常只用于小型Cache。

* **组相联映射（Set Associative）**：
    * 结合了直接映射和全相联映射的优点。
    * Cache被分成多个组（Set），每个组内有多条Cache Line。
    * 主存中的一个块通过直接映射的方式映射到某个特定的组。
    * 在该组内部，这个块可以被放置在组内的任何一条Cache Line中（类似于全相联）。
    * 通常用 $N$ 路组相联（$N$-way Set Associative）来表示，其中 $N$ 是每个组中的Cache Line数量。
    * **优点**：平衡了复杂性和性能，是现代Cache最常用的映射策略。

### Cache的替换策略

当Cache已满，而新的数据块需要被加载进来时，替换策略决定了哪一个旧的Cache Line应该被淘汰。

* **最近最少使用（Least Recently Used, LRU）**：
    * 淘汰最长时间没有被访问过的Cache Line。
    * **优点**：理论上效果最好，可以最大程度地保留可能再次被访问的数据。
    * **缺点**：实现复杂，需要记录每个Cache Line的访问时间戳或维护一个访问顺序。

* **先进先出（First In First Out, FIFO）**：
    * 淘汰最先进入Cache的Cache Line。
    * **优点**：实现简单。
    * **缺点**：可能淘汰掉仍然活跃的数据，导致命中率下降。

* **随机替换（Random）**：
    * 随机选择一个Cache Line进行淘汰。
    * **优点**：实现最简单。
    * **缺点**：性能不稳定，可能出现最差情况。

* **不常用（Least Frequently Used, LFU）**：
    * 淘汰访问次数最少的Cache Line。
    * **优点**：考虑了访问频率。
    * **缺点**：实现复杂，需要维护访问计数器。

现代Cache通常倾向于使用LRU或其近似算法（如伪LRU）来平衡性能和复杂性。

## 3. Cache的读写策略

Cache的读写策略决定了当CPU读写数据时，Cache和主存之间如何进行数据同步。

### Cache的读策略

当CPU需要读取数据时，Cache控制器会首先检查Cache中是否有该数据。

* **Cache命中（Cache Hit）**：
    * 所需数据在Cache中找到。
    * CPU直接从Cache中读取数据，速度非常快。

* **Cache缺失（Cache Miss）**：
    * 所需数据不在Cache中。
    * Cache控制器会从下一级存储（通常是主存）中将包含该数据的整个Cache Line加载到Cache中。
    * 然后CPU从Cache中读取数据。这个过程会引入额外的延迟。

### Cache的写策略

当CPU写入数据时，Cache和主存之间的数据同步方式有两种主要策略：

* **写直达（Write-Through）**：
    * 当CPU写入数据时，数据同时写入Cache和主存。
    * **优点**：
        * 数据一致性高：Cache和主存中的数据始终保持同步。
        * 实现简单。
    * **缺点**：
        * 每次写操作都需要访问主存，降低了写操作的速度。
        * 可能会占用主存总线，影响其他操作。
    * 为了缓解写直达的性能问题，通常会配合**写缓冲（Write Buffer）**使用，CPU将数据写入写缓冲后即可继续执行，写缓冲异步地将数据写入主存。

* **写回（Write-Back）**：
    * 当CPU写入数据时，数据只写入Cache中对应的Cache Line。
    * 该Cache Line的“脏位（Dirty Bit）”会被置为1，表示数据已被修改。
    * 只有当该“脏”的Cache Line被替换（淘汰）出Cache时，才会被写回主存。
    * **优点**：
        * 写操作速度快，因为大部分写操作只在Cache中完成。
        * 减少了对主存的访问次数，节省了主存带宽。
    * **缺点**：
        * 数据一致性问题：Cache和主存中的数据在一段时间内可能不一致。需要复杂的机制（如 snooping 协议）来保证多核系统中的数据一致性。
        * 实现复杂：需要额外的脏位和复杂的写回逻辑。
    * 现代高性能处理器通常采用写回策略，因为它能提供更好的写性能。

除了读写策略，还需要考虑**写分配（Write Allocate）**和**非写分配（No-Write Allocate）**。

* **写分配（Write Allocate）**：
    * 当写操作发生Cache Miss时，会将主存中的对应块加载到Cache中，然后再进行写操作。
    * 通常与写回策略配合使用。

* **非写分配（No-Write Allocate）**：
    * 当写操作发生Cache Miss时，数据直接写入主存，而不加载到Cache中。
    * 通常与写直达策略配合使用。
